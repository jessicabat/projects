[
    
    {
        "title": "Payment Risk Insight Platform",
        "year": "Winter 2026",
        "tags": ["Payments", "Risk Decisioning", "Fraud Prevention", "Policy Optimization", "Independent Project"],
        "story_title": "The Network Decision Question",
        "description": "In a global payment network, every transaction must be evaluated in milliseconds: approve seamlessly, step up for review, or block to prevent fraud. This project simulates a network-scale risk decisioning system that translates model scores into concrete policy actions. The work focuses on defining approval thresholds, evaluating precision–recall tradeoffs under time-aware testing, and optimizing economic outcomes by balancing fraud loss against customer friction.",
        "stack": ["Python", "Polars (High-Performance ETL)", "XGBoost", "SHAP", "Llama 3.1 (Local via Ollama)", "Plotly", "Streamlit"],
        "result": "<strong>Impact:</strong> Identified an optimal network policy capturing 81.5% of fraudulent activity while maintaining 89.8% approval rates, maximizing net economic value under realistic operational constraints.",
        "image": "images/payment_risk_cover.png",
        "link": "https://jessicabat.github.io/payment-risk-insight-platform/",
        "extraLink": "https://github.com/jessicabat/payment-risk-insight-platform",
        "extraLinkText": "View Policy Framework & Artifacts"
    },
    {
        "title": "Wallet Risk & Behavior Analytics",
        "year": "Winter 2026",
        "tags": ["Fintech", "Fraud Detection", "Unsupervised + Supervised", "Independent Project"],
        "story_title": "The Risk Question",
        "description": "New crypto wallets launch with no track record, but platforms still need to decide: treat them like VIPs, normal users, or potential fraudsters? I combined unsupervised wallet clustering (~1.2M Ethereum wallets in BigQuery) with a supervised fraud model (ETFD dataset, XGBoost) to answer two questions: 'What kind of wallet is this?' and 'When should we step in?'. The project walks from segmentation to precision–recall tradeoffs and a cost model that translates fraud risk into concrete dollars.",
        "stack": ["Python", "BigQuery", "XGBoost", "K-Means", "SHAP", "Plotly", "Looker Studio"],
        "result": "<strong>Impact:</strong> ~75% reduction in simulated fraud loss by converting theft risk into a smaller, predictable support cost using behavior-only features.",
        "image": "images/crypto_wallet_cover.png",
        "link": "https://jessicabat.github.io/crypto-risk-scoring/",
        "extraLink": "https://github.com/jessicabat/crypto-risk-scoring",
        "extraLinkText": "View Code & README"
    },
    {
        "title": "Trading Operations Reconciliation Engine",
        "year": "Winter 2026",
        "tags": ["Fintech", "Operations Engineering", "System Design", "Independent Project"],
        "story_title": "The Integrity Question",
        "description": "In high-frequency markets, a mismatch between what the broker sees and what the internal book records isn't just a bug—it's a financial liability. I engineered a T+0 reconciliation engine to solve the 'Break' problem: catching execution discrepancies (fees, prices, phantom fills) before the trading day ends. By optimizing PostgreSQL bulk loads and designing 'Anti-Join' verification logic, the system reduces the time-to-detection from hours to seconds, turning manual fire-fighting into automated surveillance.",
        "stack": ["Python", "PostgreSQL", "Bash Scripting", "Streamlit", "SQL (Anti-Joins)", "ETL Design"],
        "result": "<strong>Impact:</strong> Achieved <15s runtime for 50k+ daily trades and 100% detection of simulated breaks, ensuring books are settlement-ready by market close.",
        "image": "images/trade_ops_cover.png",
        "link": "https://jessicabat.github.io/trade-ops-recon-platform/", 
        "extraLink": "https://github.com/jessicabat/trade-ops-recon-platform",
        "extraLinkText": "View Architecture & Code"
    },
    {
        "title": "VibeRecommender: Mood-First Audio Engine",
        "year": "Fall 2025",
        "tags": ["Recommender Systems", "Content-Based Modeling", "Music Discovery UX", "Independent Project"],
        "story_title": "The Vibe Question",
        "description": "Some days I want to tell my music app exactly what I mean: low-speech, high-energy, mid-tempo, not too acoustic — not just “chill pop”. Why this matters: Vibe Recommender explores how a pure content-based engine can turn that kind of request into a controllable listening surface. I built a 7D audio-feature engine with three modes: sliders for precise control, seed-from-song for “more like this” grounded in audio features, and Vibe Roulette, a time-of-day persona spin.",
        "stack": ["Python", "Streamlit", "Pandas & NumPy", "scikit-learn", "Weighted Cosine Similarity"],
        "result": "<strong>Feature:</strong> 'Vibe Roulette' uses datetime logic to automate context-aware playlists.",
        "image": "images/vibe_cover.png", 
        "link": "https://jessicabat.github.io/vibe-recommender/",
        "extraLink": "https://vibe-recommender.streamlit.app/",
        "extraLinkText": "Try the Live App"
    },
    {
        "title": "Forecasting Volatility: The BERT vs. FinBERT Experiment",
        "year": "Fall 2025",
        "tags": ["NLP", "Transformers", "Fintech", "PyTorch", "Group Project"],
        "story_title": "The Specialist Trap",
        "description": "Industry logic suggests specialized models (FinBERT) always outperform generic ones. We challenged this assumption by fine-tuning both architectures to classify short-term stock volatility from unstructured headlines. The results were counter-intuitive: the financial-specific model developed a severe 'Optimism Bias' (High Recall, Low Precision), aggressively predicting growth even during downturns. The generic BERT acted as a better risk manager, ignoring noise to deliver superior risk-adjusted returns.",
        "stack": ["Python", "PyTorch", "Hugging Face Transformers", "FinBERT", "Scikit-Learn", "Matplotlib", "Pandas"],
        "result": "<strong>Discovery:</strong> Falsified the domain-superiority hypothesis. Generic BERT beat the specialized model in stability (Sharpe 0.010) by minimizing false positives.",
        "image": "images/bert_viz.gif"
    },
    {
        "title": "The Commitment Engine: Predicting Non-Refundable Playtime",
        "year": "Fall 2025",
        "tags": ["Recommender Systems", "Behavioral Analytics", "Imbalanced Learning", "Group Project"],
        "story_title": "The Refund Dilemma",
        "description": "On Steam, a purchase isn't a success if it triggers a refund (playtime < 2 hours). Most systems optimize for sales, ignoring this risk. We built a prediction engine designed to maximize 'Commitment' rather than just clicks. I architected a Hybrid Negative Sampling pipeline that feeds the model both 'Soft Negatives' (random unowned) and 'Hard Negatives' (abandoned purchases), allowing the model to distinguish between a future favorite game and a case of buyer's regret.",
        "stack": ["XGBoost", "Hybrid Sampling", "ETL Pipeline", "Precision-Recall Optimization"],
        "result": "<strong>Outcome:</strong> Processed 5.15M interactions to achieve 0.88 AUC, successfully predicting refund-proof engagement.",
        "image": "images/steam_preview.png"
    },
    {
        "title": "Neural Earth: Hybrid Climate Emulation",
        "year": "Spring 2025",
        "tags": ["Deep Learning", "Scientific ML", "Research", "Independent Project"],
        "story_title": "The Physics Bottleneck",
        "description": "Physics-based climate models are computationally prohibitive for rapid scenario testing. I engineered a Hybrid U-Net + FNO architecture to bridge this gap, combining U-Net's local feature extraction with the Fourier Neural Operator's global spectral processing. To train this deep, memory-intensive model, I implemented Mixed-Precision (FP16) training and custom gradient clipping, enabling high-fidelity 100-year climate projections in seconds.",
        "stack": ["PyTorch", "Fourier Neural Ops", "Mixed Precision", "HPC"],
        "result": "<strong>Performance:</strong> Achieved 1.31 RMSE on unseen future scenarios (SSP245), ranking in the Top 25% of the competition.",
        "image": "images/climate.png", 
        "link": "https://github.com/jessicabat/climate-emulation/blob/main/README.md",
        "extraLink": "https://github.com/jessicabat/projects/blob/main/assets/climate_report.pdf",
        "extraLinkText": "View Project Report"
    },
    {
        "title": "XGBoost Algorithmic Trading",
        "year": "Winter 2025",
        "tags": ["Fintech", "Time-Series Forecasting", "Algorithmic Trading", "Independent Project"],
        "story_title": "Signal vs. Noise",
        "description": "Technical analysis often relies on subjective chart reading. I engineered an automated trading system that streams raw market data via the yfinance API and calculates 15+ technical indicators (RSI, MACD, Volatility). Using XGBoost, the model classifies short-term price direction, executing trades only when high-probability thresholds are met to ensure capital efficiency.",
        "stack": ["Python", "XGBoost", "yfinance API", "Pandas", "Backtesting"],
        "result": "<strong>Performance:</strong> Achieved a 3.82 Sharpe Ratio and significant alpha over Buy & Hold in historical simulations.",
        "image": "images/predictions.png",
        "link": "https://tinyurl.com/metaproj"
    },
    {
        "title": "Visualizing Algorithmic Bias in Healthcare",
        "year": "Winter 2025",
        "tags": ["LLM", "AI Ethics", "Healthcare", "Group Project"],
        "story_title": "The Ethical Insight",
        "description": "AI models are often 'black boxes' that obscure discrimination. We built an interactive dashboard exposing how opioid prescription algorithms disproportionately deny care to Asian women, highlighting the need for algorithmic transparency.",
        "stack": ["Interactive Viz", "UX Research", "Data Storytelling"],
        "result": "<strong>Finding:</strong> Visualized a 79% denial rate disparity using real-world medical datasets.",
        "image": "images/healthcare_heatmap.png"
    },
    {
        "title": "Real-Time Robotics Classification",
        "year": "Fall 2024",
        "tags": ["Computer Vision", "Robotics", "Independent Project"],
        "story_title": "The Implementation",
        "description": "Real-time classification system for robotics hardware. I scraped a custom dataset and utilized Transfer Learning (MobileNetV2) to train a lightweight model capable of distinguishing between drones, rovers, and arms with limited data.",
        "stack": ["TensorFlow", "Keras", "Web Scraping", "MobileNetV2"],
        "result": "<strong>Metrics:</strong> 56% Improved Accuracy on custom scraped dataset.",
        "image": "images/robot_arch.png" 
    },
    {
        "title": "Quantifying the 'Home Court' Advantage",
        "year": "Spring 2025",
        "tags": ["Sports Analytics", "EDA", "Statistics", "Group Project"],
        "story_title": "The Analysis",
        "description": "Does the '12th Man' actually exist? We conducted a deep statistical analysis of 20,000+ NBA and NFL games to quantify the psychological and statistical impact of home crowds on point differentials.",
        "stack": ["Python", "SciPy", "Plotly", "Statistical Testing"],
        "result": "<strong>Result:</strong> Confirmed statistical significance (p < 0.001) of crowd impact on game outcomes.",
        "image": "images/nba_viz.png"
    }
]